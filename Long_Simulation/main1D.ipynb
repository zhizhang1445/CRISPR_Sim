{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as random\n",
    "from scipy.signal import convolve\n",
    "import scipy\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from scipy import sparse\n",
    "import json\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "from antigenicWaveSimulationMethods import main as coEvoSimulation\n",
    "from formulas import compute_shift\n",
    "from coverage import elementwise_coverage\n",
    "from fitness import fitness_spacers, norm_fitness, virus_growth\n",
    "from altImmunity import immunity_loss_uniform, immunity_gain_from_kernel\n",
    "from initMethods import init_exptail, init_full_kernel, init_quarter_kernel, init_guassian, init_cond\n",
    "from supMethods import time_conv, write2json, minmax_norm\n",
    "from mutation import mutation\n",
    "from randomHGT import get_time_next_HGT, HGT_logistic_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { #parameters relevant for the equations\n",
    "        \"Nh\":                     1E5,\n",
    "        \"N0\":                     1E9, #This Will be updated by self-consitent solution\n",
    "        \"R0\":                      20, \n",
    "        \"M\":                       10, #Also L, total number of spacers\n",
    "        \"mu\":                     100, #mutation rate\n",
    "        \"gamma_shape\":             20,\n",
    "        \"Np\":                     100, #Number of Cas Protein\n",
    "        \"dc\":                      10, #Required number of complexes to activate defence\n",
    "        \"h\":                       10, #coordination coeff\n",
    "        \"r\":                     2000, #cross-reactivity kernel\n",
    "        \"beta\":                     0,\n",
    "        \"rate_HGT\":                 0,\n",
    "        \"HGT_bonus_acq_ratio\":      0,\n",
    "        \"rate_recovery\":            0,\n",
    "        \"HGT_type\":                 0,\n",
    "        \"n_spacer\":                 1,\n",
    "        \"A\":                        1,\n",
    "    }\n",
    "sim_params = { #parameters relevant for the simulation (including Inital Valuess)\n",
    "        \"continue\":                 False, #DO NOT CREATE ARBITRARY FOLDERS ONLY FOR TESTS\n",
    "        \"xdomain\":                  10000,\n",
    "        \"dim\":                          1,\n",
    "        \"dx\":                           1,\n",
    "        \"tf\":                        4000,\n",
    "        \"dt\":                           1,\n",
    "        \"dt_exact_fitness\":             1,\n",
    "        \"dt_snapshot\":                  1,\n",
    "        \"initial_mean_n\":           [0,0],\n",
    "        \"initial_mean_nh\":          [0,0],\n",
    "        \"conv_size\":                 4000,\n",
    "        \"num_threads\":                 32,\n",
    "        \"foldername\":  \"../Data_Single_Test_Det_Fit\",\n",
    "        \"seed\":                         34,\n",
    "        \"hard_N0\":                   False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, sim_params = init_cond(params, sim_params)\n",
    "foldername = sim_params[\"foldername\"]\n",
    "\n",
    "try:\n",
    "    write2json(foldername, params, sim_params)\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(foldername)\n",
    "    write2json(foldername, params, sim_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "st1 = time.time()\n",
    "n = init_guassian(params[\"N\"], sim_params, \"n\")\n",
    "nh = init_exptail(params[\"Nh\"]*params[\"M\"], params, sim_params, \"nh\")\n",
    "kernel_conv = init_quarter_kernel(params, sim_params)\n",
    "kernel_immunity = init_quarter_kernel(params, sim_params, type=\"Boltzmann\")\n",
    "ed = time.time()\n",
    "            \n",
    "nh_total = params[\"Nh\"]\n",
    "n_total = params[\"N\"]\n",
    "uc = params[\"uc\"]\n",
    "sigma = params[\"sigma\"]\n",
    "M0 = params[\"M0\"]\n",
    "t = 0\n",
    "\n",
    "with open(foldername+'/runtime_stats.txt','w') as file:\n",
    "    file.write(f't: {t}| init_functions: {time_conv(ed-st1)}| Phage Population: {n_total:.4f}| Spacer Population: {nh_total:.4f}| Uc: {uc:.4f}| sigma: {sigma:.4f}| M0: {M0:.4f} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(t < sim_params[\"tf\"]):\n",
    "\n",
    "    if t%sim_params[\"dt_snapshot\"] == 0:\n",
    "        sparse.save_npz(foldername+f\"/sp_frame_n{t}\",n.tocoo())\n",
    "        sparse.save_npz(foldername+f\"/sp_frame_nh{t}\",nh.tocoo()) \n",
    "\n",
    "    str1:float = time.time()\n",
    "    p = elementwise_coverage(nh, n, kernel_conv, params, sim_params)\n",
    "    sparse.save_npz(foldername+f\"/sp_frame_p{t}\",p.tocoo())\n",
    "    st2 = time.time()\n",
    "    f = fitness_spacers(n, nh, p, params, sim_params)\n",
    "    sparse.save_npz(foldername+f\"/sp_frame_f{t}\", f.tocoo())\n",
    "    f = norm_fitness(f, n, params, sim_params) #renormalize f\n",
    "    n = virus_growth(n, f, params, sim_params, True) #update\n",
    "            \n",
    "    if (np.sum(n) <= 0) or (np.sum(n) >= (1/2)*np.sum(nh)):\n",
    "        with open(foldername+'/runtime_stats.txt','a') as file:\n",
    "            outstring = f\"DEAD at: {t}| N: {np.sum(n)}| Coverage: {time_conv(st2-st1)}| Growth: {time_conv(st3-st2)}| Mutation: {time_conv(st4-st3)}| Immunity: {time_conv(ed-st4)}| Shift Amount: {np.linalg.norm(shift_vector)} \\n\"\n",
    "            file.write(outstring)\n",
    "\n",
    "    st3 = time.time()\n",
    "    n = mutation(n, params, sim_params)\n",
    "\n",
    "    st4 = time.time()\n",
    "    nh_prev = nh\n",
    "\n",
    "    params, sim_params, num_to_add, num_to_remove = HGT_logistic_event(t, n, params, sim_params)\n",
    "    nh_gain = immunity_gain_from_kernel(nh, n, kernel_immunity, params, sim_params, num_to_add) #update nh\n",
    "    nh = immunity_loss_uniform(nh_gain, n, params, sim_params, num_to_remove)\n",
    "            \n",
    "    diff_of_acquisition = num_to_add-num_to_remove\n",
    "    shift_vector = compute_shift(nh, nh_prev, \"max\")\n",
    "    ed = time.time()\n",
    "\n",
    "    with open(foldername+'/runtime_stats.txt','a') as file:\n",
    "        M = params[\"M\"]\n",
    "        outstring = f\"t: {t}| N: {np.sum(n)}| Coverage: {time_conv(st2-st1)}| Growth: {time_conv(st3-st2)}| Mutation: {time_conv(st4-st3)}| Immunity: {time_conv(ed-st4)}| M: {M:.4f}| Net_Acq_Diff: {diff_of_acquisition:.4f}| Shift Amount: {np.linalg.norm(shift_vector):.4f} \\n\"\n",
    "        file.write(outstring)\n",
    "\n",
    "    t += sim_params[\"dt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from initMethods import init_dict_kernel\n",
    "\n",
    "kernel_dict = init_dict_kernel(params, sim_params, type = \"coverage\", exponent = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set the number of threads for OpenBLAS/MKL\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"32\"  # Set this to the desired number of threads\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"32\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"32\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"32\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def lookup_value(val):\n",
    "    val = float(val)\n",
    "    return kernel_dict.get(val, 0.)\n",
    "\n",
    "def convolve_subset(A, nonzero_values):\n",
    "    print(\"Fuck this is happening\")\n",
    "    res = np.zeros(len_ind_n)\n",
    "\n",
    "    # dist = cdist(A, B)\n",
    "\n",
    "    for i in range(len_ind_n): #go through indexes of n\n",
    "        dist = cdist(A, B[i, :].reshape(1,2))\n",
    "        res[i] = np.dot(np.vectorize(lookup_value)(dist).squeeze(), nonzero_values)\n",
    "        # res[i] = np.dot(np.vectorize(lookup_value)(dist[:, 0]).squeeze(), nonzero_values)\n",
    "        # res[i] = np.dot(dist[:, 0], nonzero_values)\n",
    "        # dist = dist[:, 1:]\n",
    "\n",
    "Nh = params[\"Nh\"]\n",
    "M = params[\"M\"]\n",
    "# num_threads = 1\n",
    "num_threads = sim_params[\"num_threads\"]\n",
    "\n",
    "x_ind_nh, y_ind_nh = nh.nonzero()\n",
    "x_ind_n, y_ind_n = n.nonzero()\n",
    "\n",
    "A = np.array([x_ind_nh, y_ind_nh]).transpose()\n",
    "A_sets = np.array_split(A, num_threads, axis = 0)\n",
    "B = np.array([x_ind_n, y_ind_n]).transpose()\n",
    "len_ind_n = len(x_ind_n)\n",
    "\n",
    "input_h = np.divide(nh, Nh*M)\n",
    "nonzero_values = np.array(input_h[x_ind_nh, y_ind_nh].toarray()).squeeze()\n",
    "\n",
    "x_nh_sets = np.array_split(x_ind_nh, num_threads)\n",
    "y_nh_sets = np.array_split(y_ind_nh, num_threads)\n",
    "# result_values = convolve_subset(A, input_h)\n",
    "# kernel_dict_copies = [deepcopy(kernel_dict) for _ in range(num_threads)]\n",
    "\n",
    "\n",
    "\n",
    "# nonzero_values_sets = np.array_split(nonzero_values, num_threads)\n",
    "# results = Parallel(n_jobs=num_threads, backend=\"loky\")(delayed(convolve_subset)\n",
    "#         (A, nonzero_values)\n",
    "#             for A, nonzero_values\n",
    "#                 in zip(A_sets, nonzero_values_sets))\n",
    "\n",
    "# result_values = sum_parallel(results, num_threads)\n",
    "# res = scipy.sparse.dok_matrix(n.shape, dtype=float)\n",
    "# res[x_ind_n, y_ind_n] = result_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cdist\n",
    "from multiprocessing import Manager, Pool\n",
    "\n",
    "# Function to perform the dictionary lookup\n",
    "def lookup_value(shared_dict, key):\n",
    "    return shared_dict.get(key, 0)\n",
    "\n",
    "def parallel_lookup(shared_dict, keys_to_lookup):\n",
    "    # Create a pool of workers\n",
    "    with Pool(processes= num_threads) as pool:\n",
    "        # Perform parallel lookup\n",
    "        results = pool.starmap(lookup_value, [(shared_dict, key) for key in keys_to_lookup])\n",
    "    return results\n",
    "\n",
    "Nh = params[\"Nh\"]\n",
    "M = params[\"M\"]\n",
    "# num_threads = 1\n",
    "num_threads = sim_params[\"num_threads\"]\n",
    "\n",
    "x_ind_nh, y_ind_nh = nh.nonzero()\n",
    "x_ind_n, y_ind_n = n.nonzero()\n",
    "\n",
    "len_ind_n = len(x_ind_n)\n",
    "\n",
    "input_h = np.divide(nh, Nh*M)\n",
    "\n",
    "x_nh_sets = np.array_split(x_ind_nh, num_threads)\n",
    "y_nh_sets = np.array_split(y_ind_nh, num_threads)\n",
    "\n",
    "nonzero_values = np.array(input_h[x_ind_nh, y_ind_nh].toarray()).squeeze()\n",
    "nonzero_values_sets = np.array_split(nonzero_values, num_threads)\n",
    "\n",
    "\n",
    "results = parallel_lookup(kernel_dict, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"../Data/test13\"\n",
    "with open(foldername + \"/params.json\") as json_file:\n",
    "    params = json.load(json_file)\n",
    "with open(foldername + \"/sim_params.json\") as json_file:\n",
    "    sim_params = json.load(json_file)\n",
    "\n",
    "i = 999\n",
    "kernel_quarter = init_quarter_kernel(params, sim_params)\n",
    "\n",
    "n = sparse.load_npz(foldername+f\"/sp_frame_n{i}.npz\").todok()\n",
    "nh = sparse.load_npz(foldername+f\"/sp_frame_nh{i}.npz\").todok()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5da1a19760c645bef876c945e2def5171d007c0fd3f14585be32e516ddabd56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
